{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_data_info(class_nm, txt_cnt):\n",
    "    with open('../data/info.txt', 'a') as f:\n",
    "        f.write(class_nm + ': ' + str(txt_cnt) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    data_path = '../data/code25/'\n",
    "    class_dir_list = [os.listdir(data_path)]\n",
    "\n",
    "    # count = 0\n",
    "    \n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    for class_nm in os.listdir(data_path):\n",
    "        \n",
    "        class_txt_cnt = 0\n",
    "\n",
    "        for txt_file in os.listdir(data_path + class_nm):\n",
    "            if txt_file.endswith('.txt'):\n",
    "                class_txt_cnt += 1\n",
    "\n",
    "                file_path = data_path + class_nm + '/' + txt_file\n",
    "\n",
    "                with open(file_path, 'r') as file:\n",
    "                    class_label = class_nm\n",
    "\n",
    "                    strings = file.readlines()\n",
    "                    data = ' '.join(strings)\n",
    "                    \n",
    "                    data_list.append(data)\n",
    "                    class_list.append(class_label)\n",
    "\n",
    "                # count += 1\n",
    "            \n",
    "            # if count == 10: break\n",
    "        \n",
    "        # record_data_info(class_nm, class_txt_cnt)\n",
    "        # break\n",
    "    \n",
    "    data = {'sourceCode': data_list, 'classLabel': class_list}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df, data_list, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, data_list, class_list = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def split(str):\n",
    "    return [char for char in str]\n",
    "\n",
    "def tokenize(sourceCode):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_codes = []\n",
    "    ch2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    ch2idx['<pad>'] = 0\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 1\n",
    "    for code in sourceCode:\n",
    "        tokenized_code = split(code)\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_codes.append(tokenized_code)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_code:\n",
    "            if token not in ch2idx:\n",
    "                ch2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_code))\n",
    "\n",
    "    return tokenized_codes, ch2idx, max_len\n",
    "\n",
    "def encode(tokenized_codes, ch2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input of our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_code in tokenized_codes:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_code += ['<pad>'] * (max_len - len(tokenized_code))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [ch2idx.get(token) for token in tokenized_code]\n",
    "        input_ids.append(input_id)\n",
    "    \n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_sourceCodes, ch2idx, max_len = tokenize(data_list)\n",
    "input_ids = encode(tokenized_sourceCodes, ch2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
