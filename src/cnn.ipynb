{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chriskhanhtran.github.io/posts/cnn-sentence-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_data_info(class_nm, txt_cnt):\n",
    "    with open('../data/info.txt', 'a') as f:\n",
    "        f.write(class_nm + ': ' + str(txt_cnt) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_nonAscii_txt(fn):\n",
    "    with open('../data/nonAsciiTxt.txt', 'a') as f:\n",
    "        f.write(fn + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nonAscii_info(nonAsciiDict):\n",
    "    with open('../data/nonAsciiInfo.txt', 'a') as f:\n",
    "        for key in nonAsciiDict:\n",
    "            f.write(key + '\\t : ' + str(nonAsciiDict[key]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    data_path = '../data/code25/'\n",
    "    class_dir_list = [os.listdir(data_path)]\n",
    "\n",
    "    # count = 0\n",
    "    \n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    max_bytes, min_bytes = -1, 999999\n",
    "    size_dict = {}\n",
    "\n",
    "    for class_nm in os.listdir(data_path):\n",
    "        \n",
    "        class_txt_cnt = 0\n",
    "        class_flag = 0\n",
    "\n",
    "        if class_nm == '.DS_Store': continue\n",
    "\n",
    "        while class_flag != 1:\n",
    "            for txt_file in os.listdir(data_path + class_nm):\n",
    "                \n",
    "                if txt_file.endswith('.txt'):\n",
    "                    file_path = data_path + class_nm + '/' + txt_file\n",
    "\n",
    "                    # file_size = os.path.getsize(file_path)\n",
    "\n",
    "                    # if not file_size in size_dict:\n",
    "                    #     size_dict[file_size] = 1\n",
    "                    # else:\n",
    "                    #     size_dict[file_size] += 1\n",
    "\n",
    "                    # if file_size > max_bytes: max_bytes = file_size\n",
    "                    # if file_size < min_bytes: min_bytes = file_size\n",
    "                \n",
    "                    nonAsciiFile = {}\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        class_label = class_nm\n",
    "\n",
    "                        strings = file.readlines()\n",
    "                        data = ' '.join(strings)\n",
    "\n",
    "                        char_cnt = 0\n",
    "                        data_124 = ''\n",
    "\n",
    "                        for chars in data:\n",
    "                            \n",
    "                            if not chars.isascii():\n",
    "\n",
    "                                if file_path not in nonAsciiFile:\n",
    "                                    nonAsciiFile[file_path] = 1\n",
    "                                else:\n",
    "                                    nonAsciiFile[file_path] += 1\n",
    "                                \n",
    "                                record_nonAscii_txt(file_path + '\\t : ' + chars)\n",
    "                            \n",
    "                            if chars.isascii():\n",
    "                                \n",
    "                                if char_cnt != 124:\n",
    "                                    char_cnt += 1\n",
    "                                    data_124 += chars\n",
    "                                elif char_cnt == 124:\n",
    "                                    char_cnt = 0\n",
    "                                    class_txt_cnt += 1\n",
    "                                    data_list.append(data_124)\n",
    "                                    class_list.append(class_label)\n",
    "\n",
    "                                    if not len(data_124) in size_dict:\n",
    "                                        size_dict[len(data_124)] = 1\n",
    "                                    else:\n",
    "                                        size_dict[len(data_124)] += 1\n",
    "                                    \n",
    "                                    data_124 = ''\n",
    "\n",
    "                                if class_txt_cnt == 10000:\n",
    "                                    class_flag = 1\n",
    "                                    break\n",
    "                    \n",
    "                    print_nonAscii_info(nonAsciiFile)\n",
    "                \n",
    "                if class_flag == 1:\n",
    "                    break\n",
    "\n",
    "                    # count += 1\n",
    "                \n",
    "                # if count == 10: break\n",
    "                \n",
    "\n",
    "        record_data_info(class_nm, class_txt_cnt)\n",
    "        # break\n",
    "    \n",
    "    data = {'sourceCode': data_list, 'classLabel': class_list}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df, [max_bytes, min_bytes], size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, size_info, size_dict = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCode_np = df.sourceCode.values\n",
    "codeClass_np = df.classLabel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def split(str):\n",
    "    return [char for char in str]\n",
    "\n",
    "def tokenize(sourceCode):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_codes = []\n",
    "    ch2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    ch2idx['<pad>'] = 0\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 1\n",
    "    for code in sourceCode:\n",
    "        tokenized_code = split(code)\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_codes.append(tokenized_code)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_code:\n",
    "            if token not in ch2idx:\n",
    "                ch2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_code))\n",
    "\n",
    "    return tokenized_codes, ch2idx, max_len\n",
    "\n",
    "def encode(tokenized_codes, ch2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input of our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_code in tokenized_codes:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_code += ['<pad>'] * (max_len - len(tokenized_code))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [ch2idx.get(token) for token in tokenized_code]\n",
    "        input_ids.append(input_id)\n",
    "    \n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_sourceCodes, ch2idx, max_len = tokenize(sourceCode_np)\n",
    "input_ids = encode(tokenized_sourceCodes, ch2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_ch2idx(ch2idx):\n",
    "    with open('../data/ch2idx.txt', 'a') as f:\n",
    "        for key in ch2idx:\n",
    "            f.write(key + '\\t : ' + str(ch2idx[key]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_ch2idx(ch2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_encode_class(classes):\n",
    "    encoded_class = []\n",
    "    class2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for one_class in classes:\n",
    "\n",
    "        if not one_class in class2idx:\n",
    "            class2idx[one_class] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        encoded_class.append(class2idx[one_class])\n",
    "\n",
    "    return np.array(encoded_class), class2idx, len(class2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_class2idx, class2idx, num_classes = tokenize_encode_class(codeClass_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, encoded_class2idx, test_size = 0.2, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Specify batch_size\n",
    "    batch_size = 50\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = \\\n",
    "data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=20,\n",
    "                 filter_sizes=[8, 16, 32],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN_NLP class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
    "                shape (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
    "                vectors. Default: False\n",
    "            vocab_size (int): Need to be specified when not pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (List[int]): List of number of filters, has the same\n",
    "                length as `filter_sizes`. Default: [100, 100, 100]\n",
    "            n_classes (int): Number of classes. Default: 2\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
    "                (batch_size, max_sent_length)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "                n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initilize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=20,\n",
    "                    filter_sizes=[16, 32, 16],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
    "    num_filters need to be of the same length.\"\n",
    "\n",
    "    # Instantiate CNN model\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=num_classes,\n",
    "                        dropout=0.5)\n",
    "    \n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    cnn_model.to(device)\n",
    "\n",
    "    # Instantiate Adadelta optimizer\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               rho=0.95)\n",
    "\n",
    "    return cnn_model, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/sourceCodeCNN{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, title, val_dataloader=None, epochs=10):\n",
    "    \"\"\"Train the CNN model.\"\"\"\n",
    "    \n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        tot_train_loss = []\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "        train_accuracy = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "             # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            tot_train_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the accuracy rate\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            train_accuracy.append(accuracy)\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = np.mean(tot_train_loss)\n",
    "        train_accuracy = np.mean(train_accuracy)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "        writer.add_scalars(title + '-CNN Training vs. Testing Loss',\n",
    "                { 'Train' : avg_train_loss, 'Test' : val_loss },\n",
    "                epoch_i + 1)\n",
    "\n",
    "        writer.add_scalars(title + '-CNN Training vs. Testing Accuracy',\n",
    "                    { 'Train' : train_accuracy, 'Test' : val_accuracy },\n",
    "                    epoch_i + 1)\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.769421   |  1.130433  |   68.86   |   27.85  \n",
      "   2    |   1.141816   |  0.930560  |   73.98   |   26.74  \n",
      "   3    |   0.973805   |  0.849592  |   76.05   |   26.45  \n",
      "   4    |   0.875508   |  0.802258  |   77.22   |   26.55  \n",
      "   5    |   0.806371   |  0.774040  |   78.17   |   27.78  \n",
      "   6    |   0.758919   |  0.759226  |   78.58   |   28.38  \n",
      "   7    |   0.714086   |  0.746339  |   78.90   |   28.42  \n",
      "   8    |   0.679552   |  0.746618  |   78.99   |   28.54  \n",
      "   9    |   0.653451   |  0.739976  |   79.17   |   28.64  \n",
      "  10    |   0.629199   |  0.744105  |   79.26   |   28.60  \n",
      "  11    |   0.609626   |  0.749606  |   79.34   |   28.61  \n",
      "  12    |   0.592765   |  0.759283  |   79.35   |   28.61  \n",
      "  13    |   0.573638   |  0.751978  |   79.58   |   28.60  \n",
      "  14    |   0.562680   |  0.758515  |   79.80   |   28.44  \n",
      "  15    |   0.547796   |  0.756737  |   79.64   |   28.60  \n",
      "  16    |   0.537828   |  0.759977  |   79.78   |   29.24  \n",
      "  17    |   0.529515   |  0.764213  |   79.92   |   29.54  \n",
      "  18    |   0.518661   |  0.761273  |   79.53   |   29.52  \n",
      "  19    |   0.512761   |  0.770083  |   79.83   |   29.25  \n",
      "  20    |   0.503958   |  0.785457  |   79.53   |   29.11  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 79.92%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(ch2idx),\n",
    "                                      embed_dim=20,\n",
    "                                      filter_sizes=[8, 16, 32, 64],\n",
    "                                      num_filters=[100, 200, 200, 100],\n",
    "                                      learning_rate=0.25,\n",
    "                                      num_classes=len(class2idx),\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, '4Layer', val_dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.731064   |  1.085173  |   70.17   |   25.14  \n",
      "   2    |   1.101619   |  0.904257  |   74.39   |   24.90  \n",
      "   3    |   0.954443   |  0.824184  |   76.49   |   24.64  \n",
      "   4    |   0.871668   |  0.779806  |   77.88   |   25.04  \n",
      "   5    |   0.812173   |  0.758614  |   78.62   |   24.65  \n",
      "   6    |   0.767980   |  0.735917  |   79.03   |   24.61  \n",
      "   7    |   0.728623   |  0.722785  |   79.68   |   24.61  \n",
      "   8    |   0.706987   |  0.714215  |   80.02   |   24.60  \n",
      "   9    |   0.679369   |  0.711746  |   80.16   |   24.51  \n",
      "  10    |   0.662203   |  0.707985  |   80.36   |   24.41  \n",
      "  11    |   0.642123   |  0.693845  |   80.59   |   23.89  \n",
      "  12    |   0.627926   |  0.705566  |   80.50   |   24.34  \n",
      "  13    |   0.614394   |  0.694873  |   80.60   |   24.32  \n",
      "  14    |   0.603346   |  0.705708  |   80.57   |   23.82  \n",
      "  15    |   0.593763   |  0.697150  |   80.74   |   24.31  \n",
      "  16    |   0.582937   |  0.705734  |   80.98   |   24.43  \n",
      "  17    |   0.575376   |  0.697855  |   81.16   |   24.54  \n",
      "  18    |   0.567721   |  0.695989  |   80.97   |   24.48  \n",
      "  19    |   0.565853   |  0.698688  |   81.04   |   25.63  \n",
      "  20    |   0.556079   |  0.697986  |   81.08   |   24.91  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 81.16%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(ch2idx),\n",
    "                                      embed_dim=20,\n",
    "                                      filter_sizes=[8, 16, 32],\n",
    "                                      num_filters=[200, 200, 200],\n",
    "                                      learning_rate=0.25,\n",
    "                                      num_classes=len(class2idx),\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, '3Layer', val_dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.279521   |  1.672238  |   53.42   |   14.18  \n",
      "   2    |   1.829512   |  1.459078  |   59.00   |   14.81  \n",
      "   3    |   1.702149   |  1.358563  |   62.00   |   14.22  \n",
      "   4    |   1.640589   |  1.298653  |   63.15   |   13.73  \n",
      "   5    |   1.599665   |  1.260498  |   64.05   |   13.73  \n",
      "   6    |   1.568060   |  1.234646  |   65.04   |   13.72  \n",
      "   7    |   1.547400   |  1.218254  |   65.86   |   13.77  \n",
      "   8    |   1.533308   |  1.208777  |   65.86   |   13.93  \n",
      "   9    |   1.517105   |  1.186603  |   66.33   |   13.91  \n",
      "  10    |   1.511194   |  1.184452  |   66.44   |   13.90  \n",
      "  11    |   1.501415   |  1.170480  |   66.62   |   14.45  \n",
      "  12    |   1.496626   |  1.172712  |   66.81   |   15.54  \n",
      "  13    |   1.486659   |  1.165691  |   66.71   |   14.02  \n",
      "  14    |   1.484627   |  1.160844  |   66.89   |   13.95  \n",
      "  15    |   1.478041   |  1.156030  |   67.05   |   13.84  \n",
      "  16    |   1.474588   |  1.149346  |   67.20   |   13.86  \n",
      "  17    |   1.469369   |  1.144989  |   67.55   |   13.80  \n",
      "  18    |   1.465827   |  1.140977  |   67.48   |   13.74  \n",
      "  19    |   1.460319   |  1.138898  |   67.56   |   13.63  \n",
      "  20    |   1.463732   |  1.141783  |   67.50   |   13.72  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 67.56%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(ch2idx),\n",
    "                                      embed_dim=20,\n",
    "                                      filter_sizes=[8, 16, 32],\n",
    "                                      num_filters=[20, 20, 20],\n",
    "                                      learning_rate=0.25,\n",
    "                                      num_classes=len(class2idx),\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, '3Layer_reduced_outChann', val_dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.590570   |  1.029992  |   71.04   |   59.73  \n",
      "   2    |   0.981587   |  0.812411  |   77.27   |   60.37  \n",
      "   3    |   0.794220   |  0.727556  |   79.46   |   60.08  \n",
      "   4    |   0.681105   |  0.681345  |   80.80   |   60.13  \n",
      "   5    |   0.602006   |  0.660076  |   81.60   |   59.59  \n",
      "   6    |   0.542749   |  0.649782  |   81.92   |   59.64  \n",
      "   7    |   0.485958   |  0.625084  |   82.71   |   59.90  \n",
      "   8    |   0.447697   |  0.630123  |   82.90   |   59.68  \n",
      "   9    |   0.415509   |  0.638942  |   82.87   |   59.44  \n",
      "  10    |   0.385800   |  0.637808  |   83.31   |   59.66  \n",
      "  11    |   0.363545   |  0.651158  |   83.31   |   59.59  \n",
      "  12    |   0.343670   |  0.641714  |   83.49   |   59.46  \n",
      "  13    |   0.325833   |  0.641075  |   83.46   |   59.43  \n",
      "  14    |   0.309193   |  0.673468  |   83.56   |   59.42  \n",
      "  15    |   0.296250   |  0.661764  |   83.71   |   59.30  \n",
      "  16    |   0.285717   |  0.664656  |   83.85   |   59.31  \n",
      "  17    |   0.269312   |  0.691107  |   83.78   |   59.04  \n",
      "  18    |   0.263436   |  0.697850  |   83.86   |   59.24  \n",
      "  19    |   0.252713   |  0.695867  |   83.83   |   59.56  \n",
      "  20    |   0.243965   |  0.718000  |   83.77   |   59.29  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.86%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(ch2idx),\n",
    "                                      embed_dim=100,\n",
    "                                      filter_sizes=[8, 16, 32],\n",
    "                                      num_filters=[200, 200, 200],\n",
    "                                      learning_rate=0.25,\n",
    "                                      num_classes=len(class2idx),\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, '3Layer-enb100dim', val_dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.620545   |  1.043980  |   70.50   |   79.54  \n",
      "   2    |   0.987620   |  0.828307  |   76.86   |   79.63  \n",
      "   3    |   0.777868   |  0.734381  |   79.29   |   79.01  \n",
      "   4    |   0.648334   |  0.692484  |   80.57   |   78.65  \n",
      "   5    |   0.551672   |  0.678767  |   81.09   |   78.40  \n",
      "   6    |   0.480520   |  0.664155  |   81.75   |   78.47  \n",
      "   7    |   0.419008   |  0.672619  |   81.90   |   78.81  \n",
      "   8    |   0.374468   |  0.674210  |   82.30   |   78.71  \n",
      "   9    |   0.334229   |  0.672682  |   82.19   |   78.71  \n",
      "  10    |   0.306661   |  0.684437  |   82.18   |   78.72  \n",
      "  11    |   0.281320   |  0.718760  |   82.41   |   78.50  \n",
      "  12    |   0.260557   |  0.723911  |   82.28   |   78.63  \n",
      "  13    |   0.239613   |  0.730185  |   82.60   |   78.71  \n",
      "  14    |   0.226987   |  0.757476  |   82.50   |   78.68  \n",
      "  15    |   0.212581   |  0.742542  |   82.76   |   78.65  \n",
      "  16    |   0.204811   |  0.780282  |   82.46   |   78.65  \n",
      "  17    |   0.196116   |  0.774567  |   82.82   |   78.70  \n",
      "  18    |   0.185439   |  0.774087  |   82.90   |   78.68  \n",
      "  19    |   0.176512   |  0.802955  |   82.78   |   78.69  \n",
      "  20    |   0.173207   |  0.806153  |   82.85   |   78.66  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 82.90%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(ch2idx),\n",
    "                                      embed_dim=100,\n",
    "                                      filter_sizes=[8, 16, 32, 64],\n",
    "                                      num_filters=[100, 200, 200, 100],\n",
    "                                      learning_rate=0.25,\n",
    "                                      num_classes=len(class2idx),\n",
    "                                      dropout=0.5)\n",
    "train(cnn_rand, optimizer, train_dataloader, '4Layer-enb100dim', val_dataloader, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348b9cd948ce87438be2e622031b2ecfa29bc2d3ecc0fd03127b9a24b30227df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
